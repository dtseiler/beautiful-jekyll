---
layout: post
title: A Sub-Prime I/O Primer
date: '2007-12-19T14:57:00.000-06:00'
author: Don Seiler
tags: 
modified_time: '2010-11-20T12:19:13.666-06:00'
blogger_id: tag:blogger.com,1999:blog-7032512792942232766.post-5325987145028808033
blogger_orig_url: http://www.seiler.us/2007/12/sub-prime-io-primer.html
---

My fans on oracle-l already know that I've had a bit of a battle with Oracle I/O recently, most of the damage being self-inflicted.  I'd like to give as forensic a review as possible so that those poor souls who come after me will have some hope and inspiration to carry on.<br /><br />First of all, some definitions, as they relate to Oracle:<br /><br />Direct I/O:  This is I/O done without the use of the OS (or filesystem) buffer cache.  Oracle already has its own buffer cache, so the filesystem cache is (usually) makes I/O take much longer than just bypassing it and relying solely on Oracle to handle the data buffer cache, which it should do much better (for it's own needs) than the filesystem.<br /><br />Asynchronous I/O:  This type of I/O means that I/O commands are sent but other processing can continue before the I/O has finished.<br /><br />NOTE: These two kinds of I/O, direct and asynchronous, are completely independent of each other.  For some reason that I can't explain, I had it stuck in my <a href="http://www.coca-cola.com/">Coke</a>-addled mind that direct was the opposite of asynch I/O.  As many times as I read over guides and glossaries, it wouldn't shake loose.<br /><br />And now, for our feature presentation.<br /><div align="center"><br /></div><div align="center"><b>Act I</b></div><div align="center"><i>In which our hero becomes bewildered with the possibilities</i></div><div align="left">I had been idling in <a href="irc://chat.freenode.net/oracle">#oracle</a> when <a href="http://halisway.blogspot.com/">hali</a> mentioned something to someone about direct I/O.  Having not dealt with I/O very much in my own travels, I inquired more.  It was then that I learned of <a href="http://download.oracle.com/docs/cd/B19306_01/server.102/b14237/initparams072.htm#sthref290">filesystemio_options</a> and the benefits of setting them to "directio."</div><div align="left">Needless to say, I was sold.</div><div align="left">I set about with my trusty system/storage administator on mounting our vxfs partitions with the proper parameters (mincache=direct,convosync=direct) and setting filesystemio_options=directio, which I'm led to believe is redundant.  Once the filesystem is mounted with the directio options, Oracle (and anything else) will always use direct I/O.</div><div align="center"><br /></div><div align="center"><b>Act II</b></div><div align="center"><i>In which direct meets asynchronous</i></div><div align="left">Fast forward three or four weeks and I'm grasping at straws trying to figure out a problem that causes the production instance to hang.  (I'm not going to address these hangs in this post, as I'm not really sure that they have to do with direct I/O.)  There seemed to be a perfect storm of changes in the weeks prior to these problems happening</div><br /><ol><li>Migrated to a 64-bit server (via datapump exp/imp)</li><li>Much larger SGA (from 1.5 Gb to 16 Gb)</li><li>Dramatically different datafile layout</li><li>Using direct I/O</li></ol>And that's just to name the ones I can think of at this moment.  I decided (after weeks of tinkering elsewhere) to look at direct I/O.  It was during this second look that I realized that one probably should also have asynchronous I/O enabled.<br /><div align="center"><br /></div><div align="center"><b>Act III</b></div><div align="center"><i>Paradise Lost </i></div>And so I set filesystemio_options=setall.  There were no problems upon instance restart.  However this showed up in that evenings RMAN backup:<br /><pre><span style="color: green;">Starting Control File and SPFILE Autobackup at 29-NOV-07<br />RMAN-00571: ===========================================================<br />RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS ===============</span><span style="color: green;"><br />RMAN-00571: ===========================================================<br /></span><br /><div class="ArwC7c ckChnd"><span style="color: green;">RMAN-03009: failure of Control File and SPFILE Autobackup command on ORA_DISK_1<br />channel at 11/29/2007 02:00:46<br />ORA-19502: write error on file "/rman/c-3171457975-20071129</span><span style="color: green;">-00",<br />blockno 321 (blocksize=16384)<br /><b>ORA-27061: waiting for async I/Os failed</b><br />Linux-x86_64 Error: 14: Bad address<br />Additional information: -1<br />Additional information: 1048576</span></div></pre><div class="ArwC7c ckChnd"></div><div class="ArwC7c ckChnd">I was seeing similar ORA errors about async I/O in my alert log intermittently when redo logs were being archived.  Obviously this wasn't good.  Google <a href="http://www.dizwell.com/prod/node/233">turned up one reference</a>, which proved to be gold.  The VxFS parameter <b>discovered_direct_iosz</b> was set to a default of 256k.  We bumped this up to 1024k and the problems disappeared, AS IF BY MAGIC!  <a href="http://structureddata.org/">Greg Rahn</a> also suggested setting <b>max_direct_iosz</b> to 1024k, as well as setting <b>vxio:vol_maxio</b>=2048 to allow 1MB max I/Os.<br /><br /></div><div class="ArwC7c ckChnd"></div><div class="ArwC7c ckChnd">Greg was also under the impression that VxFS without ODM would not perform asynchronous I/O.  He suggested I run an "strace -c" on the LGWR pid, saying that "if you see io_submit and io_getevents in the syscall column, it is using async io on Linux.  If you see pwrite64 it is not," and I was happy to see that we were indeed.</div><div class="ArwC7c ckChnd"></div><div class="ArwC7c ckChnd"><pre><span style="color: green;">oracle:~/sr $ strace -c -p 12605<br />Process 12605 attached - interrupt to quit<br />Process 12605 detached<br />% time     seconds  usecs/call     calls    errors syscall<br />------ ----------- ----------- --------- --------- ----------------<br />80.66    1.142811         298      3831           io_getevents<br />7.26    0.102820          51      2004           io_submit </span></pre></div><div align="center"></div><div align="center"><b></b></div><div align="center"><b>Act IV</b></div><div align="center"><i>Jumping to Conclusions </i></div>As I mentioned before, I'm not convinced at all that the hangings had anything to do with the I/O issues.  However I must note that since we enabled asynchronous I/O on top of the direct I/O, we haven't had any instance hanging.  Where we used to get them once every other day, it has been three weeks since the last incident.<br /><br />To their end, Oracle support had analyzed our system state dumps and noted that our session cached cursors was at 100% utilization, and a lot of the wait events during the hanging had to do with cursors (<span class="nfakPe">cursor</span>: <span class="nfakPe">pin</span> S wait on X).  Per their recommendation, we have raised the session_cached_cursors parameter, but haven't had a window yet in which to bounce the instance so the parameter can take effect.<br /><br />If there ever is a point where we figure out exactly what was causing the problem and what the solution was, I'll be sure to write about it in this space.   For now I just wanted to show you what an impulsive, ignorant fool I am.<br /><div align="center"><i>~ Fin ~</i></div>