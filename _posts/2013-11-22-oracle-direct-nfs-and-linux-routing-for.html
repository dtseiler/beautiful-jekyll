---
layout: post
title: Oracle Direct NFS (and Linux Routing) ... for Dummies!
date: '2013-11-22T15:50:00.000-06:00'
author: Don Seiler
tags:
- 11g
- nfs
- dnfs
- 11gR2
- linux
- routing
- infiniband
- networking
- oracle
modified_time: '2013-11-22T15:50:03.071-06:00'
blogger_id: tag:blogger.com,1999:blog-7032512792942232766.post-8491345739986610272
blogger_orig_url: http://www.seiler.us/2013/11/oracle-direct-nfs-and-linux-routing-for.html
---

<span style="font-family: inherit;">When I started my current role a few months ago, I was very interested to learn that direction had been set to migrate away from ASM and onto NFS storage that had some read flash cache in front of it. I'm not the world's biggest fan of ASM and the Grid Infrastructure management overhead that it requires, but once it's up and running it has been fairly solid for me. However I do like to have direct access to the files (although that can be dangerous as well), and so I was excited to see how this would go.</span><br /><div><span style="font-family: inherit;"><br /></span></div><div><span style="font-family: inherit;">However, the configuration was not quite as straight-forward as we had assumed. In this post I'd like to walk though the points where we went off course and fill in some of the gaps that I couldn't find in the documentation or other blog posts.</span></div><div><span style="font-family: inherit;"><br /></span></div><div><a name='more'></a><h2><span style="font-family: inherit; font-size: small;">Direct NFS</span></h2></div><span style="font-family: inherit;">I won't explain what Oracle Direct NFS is, other than to say that you want it if you're using NFS to host your datafiles and/or backups. Furthermore, if you're going to use Direct NFS, then you'll want to review MOS Doc ID 1495104.1 for a list of recommended patches. These patches can yield some great performance improvements in addition to fixing a few known bugs.&nbsp;</span><span style="font-family: inherit;">As far as the NFS definitions go in /etc/fstab, they are still required. As far as the NFS mount options, I'll refer you to MOS</span><span style="font-family: inherit; line-height: 19px;">&nbsp;Doc ID&nbsp;</span><span style="background-color: white; font-family: inherit; line-height: 19px;">359515.1.</span><br /><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;"><br /></span></span><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;">There are also plenty of other blog posts detailing how to enable DNFS and configure your oranfstab. I won't repeat that info here. I want to focus on the issue that I specifically ran into.</span></span><br /><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;"><br /></span></span><br /><h2><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;">Splitting the Pools</span></span></h2><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;"><br /></span></span><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;">Our NFS solution would be two have two different pools of disk handled by different controllers. One pool would be for the faster 10K RPM disks for production, and the other for slower 7200 RPM disks for development, staging, backups, etc. We had Infiniband connections going from the database host to the NFS servers on a private network dedicated just for this traffic. We'll refer to these servers by their DNS entries fast-ib and slow-ib.</span></span><span style="background-color: white; color: #575757; font-family: 'Helvetica Neue', Helvetica, Arial, 'Lucida Grande', sans-serif; font-size: 13px; line-height: 19px;"><br /></span><br /><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;"><br /></span></span><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;">The idea then is for the Direct NFS connections to use those dedicated Infiniband pipes to access the NFS shares that we need. In the case that we want to copy production datafiles to a share for staging, we'll have mounts from both NFS servers mounted on the database host. This is where I learned a couple of things about Linux routing ...</span></span><br /><span style="background-color: white; line-height: 19px;"><span style="font-family: inherit;"><br /></span></span><span style="line-height: 19px;">The NFS server Inifiniband ports were configured like this:</span><br /><br /><ul><li><span style="line-height: 19px;">fast-ib: 192.168.200.1</span></li><li><span style="line-height: 19px;">slow-ib: 192.168.200.2</span></li></ul><div><span style="line-height: 19px;">The DB host had 2 dual-port infiniband cards as well, configured like this:</span></div><div><ul><li><span style="line-height: 19px;">ib0: 192.168.200.100</span></li><li><span style="line-height: 19px;">ib1: 192.168.200.101</span></li><li><span style="line-height: 19px;">ib2: 192.168.200.102</span></li><li><span style="line-height: 19px;">ib3: 192.168.200.103</span></li></ul><div><span style="line-height: 19px;">The idea was to have ib0 be our conduit to fast-ib, and for ib1 to be our conduit to slow-ib. So our /etc/oranfstab was configured as such, based on what I had seen from other blog posts about oranfstab and DirectNFS:</span></div></div><div><span style="line-height: 19px;"><br /></span></div><div></div><br /><pre style="background-color: white; color: #222222; white-space: pre-wrap;">server: fast-ib<br />local: 192.168.200.100<br />path: 192.168.200.1<br />export: /export/fast mount:/mnt/fast<br /><br />server: slow-ib<br />local: 192.168.200.101<br />path: 192.168.200.2<br />export: /export/slow mount:/mnt/slow</pre><pre style="background-color: white; color: #222222; white-space: pre-wrap;"></pre><br />However what we saw was that any attempt to mount a database from the slow shares would hang and then core dump. Commenting out the local specification fixed this, but then everything went out of ib3 on the DB host, meaning we didn't have that segregation of traffic that we wanted.<br /><br />At first I thought it was some complication with DirectNFS and oranfstab syntax that I was missing as I read and re-read the Oracle documentation and various blog posts. Turns out it was something a bit more basic.<br /><br />After putting some chum out via Twitter, I got a big bite from Freek d'Hooge, <a class="vt-p" href="https://twitter.com/dhoogfr/status/384718307619663872" target="_blank">who quickly recognized it as a common Linux routing comprehension (or lack thereof, on my part) issue</a>. To highlight the issue for those skimming this article:<br /><br /><blockquote class="tr_bq"><span style="font-size: large;">On Linux, if you want to have segregated traffic on separate network interfaces (not load-balancing), those interfaces must be on separate subnets.</span></blockquote><br /><span style="font-family: inherit;">In our case, everything was in the same 192.168.200.255 subnet, and so could only use one interface. In our case it was using ib3 because it was listed first in the routing table, visible by running "route -n":</span><br /><span style="font-family: inherit;"><br /></span><span style="font-family: Courier New, Courier, monospace;"><span style="background-color: white; color: #222222;"># route -n | grep "Destination\|ib"</span></span><br /><br /><span style="font-family: Courier New, Courier, monospace;"><span style="background-color: white; color: #222222;">Destination&nbsp;&nbsp;&nbsp;&nbsp; Gateway&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Genmask&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Flags Metric Ref&nbsp;&nbsp;&nbsp; Use Iface</span></span><br /><span style="font-family: Courier New, Courier, monospace;"><span style="background-color: white; color: #222222;">192.168.200.0&nbsp;&nbsp; 0.0.0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 255.255.255.0&nbsp;&nbsp; U&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 ib3</span></span><br /><span style="font-family: Courier New, Courier, monospace;"><span style="background-color: white; color: #222222;">192.168.200.0&nbsp;&nbsp; 0.0.0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 255.255.255.0&nbsp;&nbsp; U&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 ib0</span></span><br /><span style="font-family: Courier New, Courier, monospace;"><span style="background-color: white; color: #222222;">192.168.200.0&nbsp;&nbsp; 0.0.0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 255.255.255.0&nbsp;&nbsp; U&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 ib1</span></span><br /><span style="font-family: Courier New, Courier, monospace;"><span style="background-color: white; color: #222222;">192.168.200.0&nbsp;&nbsp; 0.0.0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 255.255.255.0&nbsp;&nbsp; U&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 ib2</span></span><br /><br /><span style="font-family: inherit;"><br /></span><span style="font-family: inherit;">Offlining (ifdown) ib3 would just cause it to pick the next highest interface in that subnet from the routing table, in our case at this time it would have been ib0.<span style="font-family: inherit;"> Freek also mentioned that w</span></span><span style="background-color: white; color: #222222;"><span style="font-family: inherit;">ith dNFS on Unix, you can add the dontroute clause in oranfstab to avoid this, but on Linux this option does not work.</span></span><br /><span style="font-family: inherit;"><br /></span>Adding to the original misery was that our oranfstab file was trying to force traffic onto the local interfaces that weren't routing traffic (ib0 and ib1). When we commented out the "local" lines in oranfstab, then Direct NFS just used what the route table gave it for whatever subnet it needed.<br /><br />Armed with this new knowledge, we changed our NIC configurations, moving the slow-ib traffic to a different subnet:<br /><br /><span style="line-height: 19px;">The NFS server ports are now configured like this:</span><br /><ul><li><span style="line-height: 19px;">fast-ib: 192.168.200.1</span></li><li><span style="line-height: 19px;">slow-ib: 192.168.201.2</span></li></ul><div><span style="line-height: 19px;">The DB interfaces are now configured like this:</span></div><div><ul><li><span style="line-height: 19px;">ib0: 192.168.200.100</span></li><li><span style="line-height: 19px;">ib1: 192.168.201.101</span></li><li><span style="line-height: 19px;">ib2: [offline]</span></li><li><span style="line-height: 19px;">ib3: [offline]</span></li></ul><div><span style="line-height: 19px;">We could choose to bond the 4 IB interfaces pairs to increase bandwidth, but for the time being we only had one IB active on each NFS server anyway so the bottleneck would be the same. Moving on, though, this is what the oranfstab looks like now in our working configuration:</span></div></div><div><span style="line-height: 19px;"><br /></span></div><div><pre style="background-color: white; color: #222222; white-space: pre-wrap;">server: fast-ib</pre><pre style="background-color: white; color: #222222; white-space: pre-wrap;">local: 192.168.200.100<br />path: 192.168.200.1<br />export: /export/fast mount:/mnt/fast<br /><br />server: slow-ib</pre><pre style="background-color: white; color: #222222; white-space: pre-wrap;">local: 192.168.201.101<br />path: 192.168.201.2<br />export: /export/slow mount:/mnt/slow</pre><pre style="background-color: white; color: #222222; white-space: pre-wrap;"></pre><pre style="background-color: white; color: #222222; white-space: pre-wrap;"></pre><span style="font-family: inherit;"><br /></span><span style="font-family: inherit;">Since we disabled ib2 and ib3, we now know for certain which interface will be used for each server. This means we also now know the IP address and can specify that in the "local" parameter line. We could have left ib2 &amp; ib3 enabled and relied again on default Linux routing, but I prefer to eliminate variables whenever possible.</span><br /><span style="font-family: inherit;"><br /></span><span style="font-family: inherit;">So now we have our production database files on /mnt/fast and our slower-is-somewhat-acceptable database files for staging and development on /mnt/slow. Traffic to each one is on a dedicated Infiniband port that is routed separated from the other.</span><br /><span style="font-family: inherit;"><br /></span>Of course, it hasn't been all milk and honey. Next week I'll be back with some of the issues we've encountered actually using the Infiniband and the adjustments and fixes that were made to overcome them.</div>