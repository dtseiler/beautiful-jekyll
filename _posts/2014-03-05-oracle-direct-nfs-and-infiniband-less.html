---
layout: post
title: 'Oracle Direct NFS and Infiniband: A Less-Than-Perfect Match'
date: '2014-03-05T19:54:00.001-06:00'
author: Don Seiler
tags:
- redhat
- oel
- oracle
- rhel
- uek
- 11g
- bugs
- nfs
- dnfs
- 11gR2
- linux
- infiniband
- networking
- zfssa
modified_time: '2014-03-06T14:45:14.066-06:00'
blogger_id: tag:blogger.com,1999:blog-7032512792942232766.post-5596249186870877294
blogger_orig_url: http://www.seiler.us/2014/03/oracle-direct-nfs-and-infiniband-less.html
---

Readers of <a class="vt-p" href="http://www.seiler.us/2013/11/oracle-direct-nfs-and-linux-routing-for.html" target="_blank">an earlier post on this blog</a> will know about my latest forays into the world of Direct NFS. Part of that means stumbling over configuration hiccups or slamming into brick walls when you find new bugs.<br /><br />To quickly re-set the table, my organization purchased the <a class="vt-p" href="http://www.oracle.com/us/products/servers-storage/storage/nas/zfs7420/overview/index.html" target="_blank">Oracle ZFS Storage Appliance (ZFSSA) 7420</a>. Oracle sold us on the Infiniband connectivity as a way to make a possible future transition to Exadata easier. However the pre-sales POC testing was done over 10gb Ethernet (10gigE). So it was that everything (including their Infiniband switches and cables) arrived at the datacenter and was installed and connected by the Oracle technicians. There were a few initial hiccups and frustrating inconsistencies with their installation and configuration, but those are outside the scope of this post.<br /><br />We decided to put a copy of our standby database on the ZFSSA and have it run as a second standby. The performance problems were quick to appear, and they weren't pretty.<br /><br /><a name='more'></a><br /><br /><h3>Configuring Shares</h3>We configured the ZFS project shares by <a class="vt-p" href="http://www.oracle.com/technetwork/server-storage/solaris/config-solaris-zfs-wp-167894.pdf" target="_blank">the common Oracle best practices in terms ZFS recordsize and write bias</a>. For example, datafile shares were set to an 8k recordsize (to match the db_block_size) and throughput write bias, where as redo log shares were set to 128k recordsize and latency bias. Note that with Oracle Database 12c, Direct NFS over NFSv4, and the more recent ZFSSA firmware, you gain the benefit of <a class="vt-p" href="http://docs.oracle.com/cd/E27998_01/html/E48433/integration__oracle_intelligent_storage_protocol.html" target="_blank">Oracle Intelligent Storage Protocol (OISP)</a>, which will determine the recordsize and write bias automatically based on the type of file it recognizes.<br /><br /><h3>Copying the Database</h3>To start out we needed to get a copy of the database onto the ZFSSA shares. This was easily done with RMAN's backup as copy database command, specifying the ZFSSA mount as the format destination. We were fairly impressed with the Direct NFS transfer speed during the copy and so we were optimistic about how it would stand up with our production load.<br /><br /><h3>Starting Recovery!</h3><div>Once everything was set, we started managed recovery on the standby. Our earlier excitement gave way to a sort of soul-crushing disappointment as the recovery performance basically ground to a standstill and traffic to the ZFSSA went from hundreds of Mbps to barely a trickle. We could stop recovery and copy a big file with great speed, but something in managed recovery was not playing nicely.<br /><br />We found that we could disable Direct NFS (requires a database restart and software relinking), and managed recovery would actually perform better over the kernel NFS, although still not nearly as well as we would need.<br /><br />This started a blizzard of SR creations, including SRs being spawned from other SRs. We had SRs open for the ZFSSA team, the Direct NFS team, the Data Guard team, and even the Oracle Linux and Solaris teams, even though we were not on Oracle Linux or Solaris (we use RHEL). It came to a point where I had to tell our account manager to have support stop creating new SRs, since every new SR meant I had to explain the situation to a new technician all over again.<br /><br />At this point we were having <i>twice-daily</i> conference calls with our account manager and technical leads from the various departments. Their minions were working hard on their end to replicate the problem and find a solution, but we were running into a 4th week of this craziness.<br /><br /><h3>The Infiniband Bandit</h3>After many frustrating weeks of changing configurations, cables, cards, and just generally grasping at straws, it was finally narrowed down to the Infiniband. Or rather, a bug in the open fabric (OFA) linux kernel module that dealt with Infiniband that was triggered when Direct NFS would fire off a whole lot of connections, like when DataGuard managed recover would fire up 80 parallel slaves. We tested out the 10gigE channel we had for the management UI and performance was like night and day with just the one channel.<br /><span style="font-family: inherit;"><br /></span><span style="font-family: inherit;">Oracle Support suggested it might be related to bug&nbsp;<span style="background-color: white; color: #222222;">15824316, which also deals with dramatic performance loss with Direct NFS over Infiniband. The bug in the OFA kernel module was fixed in recent versions of Oracle Enterprise Linux (OEL) (specifically the UEK kernel), but Oracle is <b>not </b>sharing this fix with Red Hat (or anyone else, presumably). Since we're on RHEL, we had little choice but to send all the Infiniband networking hardware back and order up some 10gigE replacements.</span></span><br /><span style="font-family: inherit;"><span style="background-color: white; color: #222222;"><br /></span></span>We're still in the process of getting the 10gigE switches and cables all in place for the final production setup. If you're curious, it's 4 10gigE cards per server, bonded to a single IP to a 10gigE switch into the ZFSSA heads. This 10gigE network is dedicated exclusively to ZFSSA traffic.<br /><br />So, in the end, if you're on (a recent version of) of OEL/UEK, you <i>should </i>have&nbsp;nothing to worry about. But if you're on RHEL and planning to use Direct NFS, you're going to want to use 10gigE and NOT Infiniband.<br /><br /><h3>Update - 6 Mar 2014</h3>Some of have asked, and I want to re-iterate: Oracle have claimed that the OFA module was entirely re-written, and their fix is specific to OEL and is not covered by GPL or any similar license. We were told that they have no plans to share their code with RHEL. Also there is no MOS bug number for the OFA issue, it was apparently re-written from scratch with no bug to track the issue. If this all sounds rather dubious to you, join the club. But it's what our account manager told us at the end of last year.<br /><br /><h3><span style="font-family: inherit;">Another Update - 6 Mar 2014</span></h3><span style="background-color: white; color: #222222;"><span style="font-family: inherit;">Bjoern Rost and I discussed this privately and after quite a bit of research and analysis he shared this conclusion:</span><span style="background-color: white; color: #222222;"><span style="font-family: inherit;"><br /></span></span></span><br /><blockquote class="tr_bq"><span style="background-color: white; color: #222222;"><span style="background-color: white; color: #222222;"><span style="font-family: inherit;">Oracle Support suggested that this issue would not occur with the OFA module used in Oracle Linux with the UEK kernel. RedHat changed their support in RHEL6 from shipping the whole openfabrics stack to just including the drivers that were also present in the upstream mainline kernel. This is RedHatâ€™s policy to ensure stability in the version of the kernel they ship. Oracle offers an OFA package with some additional patches (all GPL/BSD license) for the UEKr1 and UEKr2 kernels. Unfortunately, these two different approaches make it very hard to pinpoint specific patches or create backports for the RedHat kernel version.</span></span></span></blockquote></div>